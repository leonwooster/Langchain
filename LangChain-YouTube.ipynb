{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>YouTube audio</h2>\n",
    "<p>\n",
    "Building chat or QA applications on YouTube videos is a topic of high interest.\n",
    "\n",
    "Below we show how to easily go from a YouTube url to audio of the video to text to chat!\n",
    "\n",
    "We wil use the OpenAIWhisperParser, which will use the OpenAI Whisper API to transcribe audio to text, and the OpenAIWhisperParserLocal for local support and running on private clouds or on premise.\n",
    "\n",
    "Note: You will need to have an <b>OPENAI_API_KEY</b> supplied.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  yt_dlp\n",
    "%pip install --upgrade --quiet  pydub\n",
    "%pip install --upgrade --quiet  librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.0\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\pythonenv\\python3106_langchain\\lib\\site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-experimental\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.blob_loaders.youtube_audio import (\n",
    "    YoutubeAudioLoader,\n",
    ")\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers.audio import OpenAIWhisperParserLocal\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>YouTube url to text</h3>\n",
    "<p>\n",
    "Use YoutubeAudioLoader to fetch / download the audio files.\n",
    "Then, ues OpenAIWhisperParser() to transcribe them to text.\n",
    "Let’s take the first lecture of Andrej Karpathy’s YouTube course as an example!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/kCc8FmEb1nY\n",
      "[youtube] kCc8FmEb1nY: Downloading webpage\n",
      "[youtube] kCc8FmEb1nY: Downloading ios player API JSON\n",
      "[youtube] kCc8FmEb1nY: Downloading android player API JSON\n",
      "[youtube] kCc8FmEb1nY: Downloading m3u8 information\n",
      "[info] kCc8FmEb1nY: Downloading 1 format(s): 140\n",
      "[download] C:\\Users\\leonwoo\\Downloads\\YouTube\\Let's build GPT： from scratch, in code, spelled out..m4a has already been downloaded\n",
      "[download] 100% of  107.73MiB\n",
      "[ExtractAudio] Not converting audio C:\\Users\\leonwoo\\Downloads\\YouTube\\Let's build GPT： from scratch, in code, spelled out..m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://youtu.be/VMj-3S1tku0\n",
      "[youtube] VMj-3S1tku0: Downloading webpage\n",
      "[youtube] VMj-3S1tku0: Downloading ios player API JSON\n",
      "[youtube] VMj-3S1tku0: Downloading android player API JSON\n",
      "[youtube] VMj-3S1tku0: Downloading m3u8 information\n",
      "[info] VMj-3S1tku0: Downloading 1 format(s): 140\n",
      "[download] C:\\Users\\leonwoo\\Downloads\\YouTube\\The spelled-out intro to neural networks and backpropagation： building micrograd.m4a has already been downloaded\n",
      "[download] 100% of  134.98MiB\n",
      "[ExtractAudio] Not converting audio C:\\Users\\leonwoo\\Downloads\\YouTube\\The spelled-out intro to neural networks and backpropagation： building micrograd.m4a; file is already in target format m4a\n",
      "Transcribing part 1!\n",
      "Transcribing part 2!\n",
      "Transcribing part 3!\n",
      "Transcribing part 4!\n",
      "Transcribing part 5!\n",
      "Transcribing part 6!\n",
      "Transcribing part 1!\n",
      "Transcribing part 2!\n",
      "Transcribing part 3!\n",
      "Transcribing part 4!\n",
      "Transcribing part 5!\n",
      "Transcribing part 6!\n",
      "Transcribing part 7!\n",
      "Transcribing part 8!\n"
     ]
    }
   ],
   "source": [
    "# set a flag to switch between local and remote parsing\n",
    "# change this to True if you want to use local parsing\n",
    "local = False\n",
    "# Two Karpathy lecture videos\n",
    "urls = [\"https://youtu.be/kCc8FmEb1nY\", \"https://youtu.be/VMj-3S1tku0\"]\n",
    "\n",
    "# Directory to save audio files\n",
    "save_dir = \"~/Downloads/YouTube\"\n",
    "\n",
    "# Transcribe the videos to text\n",
    "if local:\n",
    "    loader = GenericLoader(\n",
    "        YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParserLocal()\n",
    "    )\n",
    "else:\n",
    "    loader = GenericLoader(YoutubeAudioLoader(urls, save_dir), OpenAIWhisperParser())\n",
    "#make sure anti virus is allowed to bypass the following line of code. \n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi everyone. So by now you have probably heard of ChatGPT. It has taken the world and the AI community by storm and it is a system that allows you to interact with an AI and give it text-based tasks. So for example, we can ask ChatGPT to write us a small haiku about how important it is that people understand AI and then they can use it to improve the world and make it more prosperous. So when we run this, AI knowledge brings prosperity for all to see, embrace its power. Okay, not bad. And so you'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list of Documents, which can be easily viewed or parsed\n",
    "docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building a chat app from YouTube video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Given Documents, we can easily enable chat / question+answering.'''\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine doc\n",
    "combined_docs = [doc.page_content for doc in docs]\n",
    "text = \" \".join(combined_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split them\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "splits = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an index\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = FAISS.from_texts(splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Why do we need to zero out the gradient before backprop at each step?',\n",
       " 'result': \"We need to zero out the gradient before backpropagation at each step because the gradients accumulate during the backward pass. If we don't reset the gradients to zero, the gradients from previous iterations will continue to accumulate and affect the current iteration, leading to incorrect updates of the model parameters. By zeroing out the gradients before each backward pass, we ensure that only the gradients from the current iteration are considered for updating the model parameters.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a question!\n",
    "query = \"Why do we need to zero out the gradient before backprop at each step?\"\n",
    "qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the difference between an encoder and decoder?\"\n",
    "qa_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"For any token, what are x, k, v, and q?\"\n",
    "qa_chain.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3106_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
